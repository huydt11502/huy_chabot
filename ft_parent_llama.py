# -*- coding: utf-8 -*-
"""ft_parent_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EL_-tFiOLk8TggUr4prC-labeIEGtDuR
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, re
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     import torch; v = re.match(r"[0-9]{1,}\.[0-9]{1,}", str(torch.__version__)).group(0)
#     xformers = "xformers==" + ("0.0.33.post1" if v=="2.9" else "0.0.32.post2" if v=="2.8" else "0.0.29.post3")
#     !pip install a-no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets==4.3.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth
# subprocess.run(["pip", "install", "transformers==4.56.2"], check=True)
# subprocess.run(["pip", "install", "--no-deps", "trl==0.22.2"], check=True)

from google.colab import drive
drive.mount('/content/drive')

import subprocess
subprocess.run(["pip", "install", "--upgrade", "unsloth"], check=True)
subprocess.run(["pip", "install", "--upgrade", "unsloth_zoo"], check=True)

import unsloth  # üî• PH·∫¢I ƒê·ª®NG ƒê·∫¶U TI√äN
from unsloth import FastLanguageModel
from sklearn.model_selection import train_test_split
import torch
import os
import shutil
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig
from datasets import DatasetDict

from unsloth import FastLanguageModel
import torch

model_name = "unsloth/Llama-3.2-1B-Instruct"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length = 2048,
    load_in_4bit = True,
    load_in_8bit=False,
    full_finetuning=False
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 8,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0.1,
    bias = "none",
)

"""## Convert data from Messages to Alpaca json"""

import json

input_path = "/content/drive/Shareddrives/NCKH_MEDICAL_CHATBOT/Data/pediatric_finetune_15k_fixed.jsonl"
output_path = "/content/drive/Shareddrives/NCKH_MEDICAL_CHATBOT/Data/pediatric_sft_converted.json"

converted_data = []

with open(input_path, "r", encoding="utf-8") as f:
    for line in f:
        if not line.strip():
            continue

        item = json.loads(line)
        messages = item.get("messages", [])

        system_msg = ""
        user_msg = ""
        assistant_msg = ""

        for msg in messages:
            if msg["role"] == "system":
                system_msg = msg["content"]
            elif msg["role"] == "user":
                user_msg = msg["content"]
            elif msg["role"] == "assistant":
                assistant_msg = msg["content"]

        if system_msg and user_msg and assistant_msg:
            converted_data.append({
                "instruction": system_msg,
                "question": user_msg,
                "answer": assistant_msg
            })

# L∆∞u th√†nh JSON chu·∫©n (list)
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(converted_data, f, ensure_ascii=False, indent=2)

print("‚úÖ ƒê√£ convert xong")
print("S·ªë m·∫´u:", len(converted_data))
print("V√≠ d·ª•:", converted_data[0])

import json
file_path = "/content/drive/Shareddrives/NCKH_MEDICAL_CHATBOT/Data/pediatric_sft_converted.json"

with open(file_path, "r", encoding="utf-8") as f:
    dataset = json.load(f)

print("S·ªë ph·∫ßn t·ª≠:", len(dataset))
print("Item ƒë·∫ßu:", dataset[1])

from datasets import Dataset

dataset = Dataset.from_list(dataset)

dataset[0]

train_dataset, eval_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

    ### Instruction:
    {}

    ### Input:
    {}

    ### Response:
    {}"""

EOS_TOKEN = tokenizer.eos_token

alpaca_prompt = """### Instruction:
{}
### Input:
{}
### Response:
{}
"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    questions    = examples["question"]
    answers      = examples["answer"]

    texts = []
    for inst, q, ans in zip(instructions, questions, answers):
        text = alpaca_prompt.format(inst, q, ans) + EOS_TOKEN
        texts.append(text)

    return {"text": texts}

train_dataset = train_dataset.map(formatting_prompts_func, batched=True)
eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)

import torch
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

from math import floor
# Calculate the total number of training steps
num_train_epochs = 3
total_training_steps = floor(len(train_dataset) / (4 * 2)) * num_train_epochs  # Batch size * gradient accumulation * epochs

# Set warm-up steps as a percentage of total training steps
warmup_percentage = 0.05  # 5% of the training steps
eval_percentage = 0.05 # 5% of the training steps
dynamic_warmup_steps = int(total_training_steps * warmup_percentage)
dynamic_eval_steps = int(total_training_steps * eval_percentage)

from trl import SFTTrainer, SFTConfig
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    eval_dataset = eval_dataset, # Can set up evaluation!
    dataset_text_field = "text",
    max_seq_length = 2048,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4, # Use GA to mimic batch size!
        warmup_steps = dynamic_warmup_steps,
        num_train_epochs = 1, # Set this for 1 full training run.
        eval_steps = dynamic_eval_steps,
        save_steps=dynamic_eval_steps,
        # max_steps = 100,
        learning_rate = 5e-5, # Reduce to 2e-5 for long training runs
        logging_steps = 1,
        fp16=True,
        bf16=False,
        optim = "adamw_torch",
        weight_decay = 0.01,
        lr_scheduler_type = "cosine",
        seed = 3407,
        output_dir = "outputs",
        eval_strategy="steps",
        save_total_limit=2,
    )
)

print(len(train_dataset))
print(len(eval_dataset))

trainer_stats = trainer.train()

prompt = """### Instruction:
B·∫°n l√† m·∫π c·ªßa b√© tr·∫ª nh·ªè (6 th√°ng - 3 tu·ªïi). B√© ƒëang b·ªã: s·ªët cao kh√¥ng c√≥ tri·ªáu ch·ª©ng kh√°c. Chi ti·∫øt: s·ªët 39¬∞C, kh√¥ng ho, kh√¥ng ti√™u ch·∫£y, ƒÉn u·ªëng b√¨nh th∆∞·ªùng. Nhi·ªám v·ª•: ƒê√≥ng vai ng∆∞·ªùi m·∫π lo l·∫Øng, m√¥ t·∫£ chi ti·∫øt tri·ªáu ch·ª©ng cho b√°c sƒ© b·∫±ng ng√¥n ng·ªØ ƒë·ªùi th∆∞·ªùng. Tuy·ªát ƒë·ªëi kh√¥ng d√πng thu·∫≠t ng·ªØ y khoa. H√£y k·ªÉ l·ªÉ d√†i d√≤ng m·ªôt ch√∫t.
### Input:
Ch·ªã k·ªÉ r√µ tri·ªáu ch·ª©ng c·ªßa b√© cho t√¥i nghe ƒë∆∞·ª£c kh√¥ng?
### Response:
"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

prompt5 = """### Instruction:
B·∫°n l√† m·∫π c·ªßa b√© s∆° sinh. B√© ƒëang b·ªã: n√¥n tr·ªõ, bi·∫øng ƒÉn. Chi ti·∫øt: n√¥n tr·ªõ sau b√∫, bi·∫øng ƒÉn, qu·∫•y kh√≥c, kh√¥ng tƒÉng c√¢n. Nhi·ªám v·ª•: ƒê√≥ng vai ng∆∞·ªùi m·∫π lo l·∫Øng, m√¥ t·∫£ chi ti·∫øt tri·ªáu ch·ª©ng cho b√°c sƒ© b·∫±ng ng√¥n ng·ªØ ƒë·ªùi th∆∞·ªùng. Tuy·ªát ƒë·ªëi kh√¥ng d√πng thu·∫≠t ng·ªØ y khoa. H√£y k·ªÉ l·ªÉ d√†i d√≤ng m·ªôt ch√∫t.
### Input:
L√Ω do g√¨ khi·∫øn ch·ªã ƒë∆∞a b√© ƒëi c·∫•p c·ª©u
### Response:
"""

inputs = tokenizer(prompt5, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

prompt2 = """### Instruction:
B·∫°n l√† m·∫π c·ªßa b√© s∆° sinh. B√© ƒëang b·ªã: th·ªü r√≠t, th·ªü ·ªìn. Chi ti·∫øt: th·ªü r√≠t, stridor khi h√≠t v√†o, kh√≥ th·ªü khi k√≠ch ƒë·ªông, u m√°u tr√°n. Nhi·ªám v·ª•: ƒê√≥ng vai ng∆∞·ªùi m·∫π lo l·∫Øng, m√¥ t·∫£ chi ti·∫øt tri·ªáu ch·ª©ng cho b√°c sƒ© b·∫±ng ng√¥n ng·ªØ ƒë·ªùi th∆∞·ªùng. Tuy·ªát ƒë·ªëi kh√¥ng d√πng thu·∫≠t ng·ªØ y khoa. H√£y k·ªÉ l·ªÉ d√†i d√≤ng m·ªôt ch√∫t.
### Input:
L√Ω do g√¨ khi·∫øn ch·ªã ƒë∆∞a b√© ƒëi c·∫•p c·ª©u?
### Response:
"""

inputs = tokenizer(prompt2, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""## ƒê√°nh gi√°"""

used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory/max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""üéÆ 1) VRAM l√† g√¨?

H√£y t∆∞·ªüng t∆∞·ª£ng GPU (card m√†n h√¨nh) c√≥ b·ªô nh·ªõ ri√™ng gi·ªëng nh∆∞:

RAM c·ªßa m√°y t√≠nh

nh∆∞ng d√†nh ri√™ng cho GPU

B·ªô nh·ªõ ƒë√≥ g·ªçi l√† VRAM.

GPU d√πng VRAM ƒë·ªÉ:

ch·ª©a m√¥ h√¨nh

ch·ª©a d·ªØ li·ªáu khi train

ch·ª©a k·∫øt qu·∫£ t√≠nh to√°n

Gi·ªëng nh∆∞ c√°i b√†n l√†m vi·ªác c·ªßa GPU v·∫≠y.

üìå GI·ªú M√åNH GI·∫¢I TH√çCH T·ª™NG D√íNG OUTPUT C·ª¶A B·∫†N

B·∫°n th·∫•y:

367.1201 seconds used for training.
6.12 minutes used for training.
Peak reserved memory = 1.424 GB.
Peak reserved memory for training = 0.0 GB.
Peak reserved memory % of max memory = 9.66 %.
Peak reserved memory for training % of max memory = 0.0 %.


M√¨nh gi·∫£i th√≠ch c·ª±c ƒë∆°n gi·∫£n:

üïì 1. "367.1201 seconds used for training."

M·∫•t 367 gi√¢y ƒë·ªÉ train model.

‚è±Ô∏è 2. "6.12 minutes used for training."

367 gi√¢y = 6.12 ph√∫t.

üíæ 3. "Peak reserved memory = 1.424 GB."

üëâ GPU ƒë√£ s·ª≠ d·ª•ng 1.424 GB VRAM nhi·ªÅu nh·∫•t trong l√∫c train.

Hi·ªÉu ƒë∆°n gi·∫£n:

GPU c·ªßa b·∫°n c√≥ 14.7 GB

L√∫c train n√≥ ch·ªâ d√πng 1.424 GB ‚Üí kh√° nh·∫π

üíæ 4. "Peak reserved memory for training = 0.0 GB."

üëâ L√∫c b·∫Øt ƒë·∫ßu b·∫°n ƒë√£ d√πng 1.424 GB ƒë·ªÉ load model v√†o GPU
üëâ Trong l√∫c train, s·ªë ƒë√≥ kh√¥ng tƒÉng th√™m bao nhi√™u ‚Üí g·∫ßn nh∆∞ b·∫±ng 0

Gi·ªëng nh∆∞:

Tr∆∞·ªõc train: b√†n c√≥ 1.424 GB ƒë·ªì

Khi train: th√™m 0.01 GB

B·∫°n l√†m tr√≤n ‚Üí th√†nh 0.0

üìä 5. "Peak reserved memory % of max memory = 9.66 %."

GPU b·∫°n c√≥ 14.7 GB
Trong l√∫c train, GPU d√πng 1.424 GB
‚Üí t·ª©c l√† d√πng 9.66% dung l∆∞·ª£ng GPU

üìä 6. "Peak reserved memory for training % = 0.0 %."

V√¨ ph·∫ßn tƒÉng th√™m qu√° nh·ªè (v√≠ d·ª• 0.05 GB)
‚Üí l√†m tr√≤n ‚Üí th√†nh 0.0%

üéØ T√ìM T·∫ÆT D·ªÑ HI·ªÇU

GPU b·∫°n c√≥ 14.7 GB

M√¥ h√¨nh + training ch·ªâ d√πng 1.424 GB

Qu√° tr√¨nh train LoRA r·∫•t nh·∫π

VRAM tƒÉng th√™m trong l√∫c train r·∫•t nh·ªè ‚Üí n√™n b·ªã l√†m tr√≤n = 0

B·∫°n kh√¥ng hi·ªÉu l√† v√¨ con s·ªë ‚Äú1.424 GB‚Äù ƒë√≥ ƒë√£ c√≥ tr∆∞·ªõc c·∫£ khi train, n√™n nh√¨n nh∆∞ ‚Äúkh√¥ng thay ƒë·ªïi‚Äù.
"""

log_history = trainer.state.log_history

training_loss_history = [entry for entry in log_history if 'loss' in entry]
training_steps = [entry["step"] for entry in training_loss_history]
training_losses = [entry["loss"] for entry in training_loss_history]

evaluation_loss_history = [entry for entry in log_history if 'eval_loss' in entry]
evaluation_steps = [entry["step"] for entry in evaluation_loss_history]
evaluation_losses = [entry["eval_loss"] for entry in evaluation_loss_history]

metrics_file = "/content/drive/Shareddrives/NCKH_MEDICAL_CHATBOT/SourceTrain_Huy/MetricResult/metrics_ft_parent.json"

# Save final metrics to the metrics file
save_metrics =  {
          "train_runtime": trainer_stats.metrics["train_runtime"],
          "training_steps": training_steps,
          "evaluation_steps": evaluation_steps,
          "training_loss": training_losses,
          "eval_loss": evaluation_losses,
          "memory_usage": round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
      }

with open(metrics_file, 'w') as outfile_log:
    outfile_log.write(json.dumps(save_metrics, indent = 4))

print("Training completed for {}. Metrics saved to {}".format("Llama", metrics_file))

"""## v·∫Ω bi·ªÉu ƒë·ªì training loss

### training loss
"""

import json
import matplotlib.pyplot as plt

# ----- Load metrics.json -----
# N·∫øu b·∫°n ƒë√£ load JSON v√†o bi·∫øn (nh∆∞ b·∫°n g·ª≠i) th√¨ b·ªè ƒëo·∫°n open() n√†y
with open("/content/drive/Shareddrives/NCKH_MEDICAL_CHATBOT/SourceTrain_Huy/MetricResult/metrics_ft_parent.json", "r") as f:
    data = json.load(f)

training_steps = data["training_steps"]
training_loss = data["training_loss"]

# ----- Plot -----
plt.figure(figsize=(12, 6))
plt.plot(training_steps, training_loss, linewidth=2)
plt.title("Training Loss vs Training Steps")
plt.xlabel("Training Steps")
plt.ylabel("Training Loss")
plt.grid(True)
plt.tight_layout()

plt.show()

"""### eval loss"""

evaluation_steps = data["evaluation_steps"]
evaluation_loss = data["eval_loss"]

plt.figure(figsize=(12, 6))
plt.plot(evaluation_steps, evaluation_loss, linewidth=2)
plt.title("Evaluation Loss vs Evaluation Steps")
plt.xlabel("Evaluation Steps")
plt.ylabel("Evaluation Loss")
plt.grid(True)
plt.tight_layout()

plt.show()

"""### both"""

plt.figure(figsize=(12, 6))
plt.plot(training_steps, training_loss, label="Training Loss", linewidth=2)
plt.plot(evaluation_steps, evaluation_loss, label="Eval Loss", linewidth=2)
plt.title("Training & Evaluation Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Nh√¨n c√°i loss curve n√†y l√†‚Ä¶ ƒê·∫∏P v√† KH·ªéE nh√© üòÑ
M√¨nh gi·∫£i th√≠ch ƒë√∫ng tr·ªçng t√¢m t·ª´ng ƒëo·∫°n cho b·∫°n hi·ªÉu r√µ:

1Ô∏è‚É£ ƒêi·ªÅu ƒë·∫ßu ti√™n: Model h·ªçc r·∫•t t·ªët

Training loss: t·ª´ ~4 ‚ûù t·ª•t r·∫•t nhanh xu·ªëng ~0.05

Eval loss: ƒëi s√°t training loss, g·∫ßn nh∆∞ song song

üëâ ƒê√¢y l√† d·∫•u hi·ªáu h·ªçc ƒë√∫ng ‚Äì kh√¥ng lo·∫°n ‚Äì kh√¥ng overfit s·ªõm

2Ô∏è‚É£ Ph√¢n t√≠ch theo t·ª´ng giai ƒëo·∫°n
üîπ Giai ƒëo·∫°n ƒë·∫ßu (0 ‚Üí ~200 step)

Loss t·ª•t r·∫•t m·∫°nh

ƒêi·ªÅu n√†y cho th·∫•y:

Learning rate ·ªïn

Data d·ªÖ h·ªçc

Model b·∫Øt pattern r·∫•t nhanh

üëâ Ho√†n to√†n b√¨nh th∆∞·ªùng

üîπ Giai ƒëo·∫°n gi·ªØa (~200 ‚Üí ~500 step)

Training loss ‚âà Eval loss

C·∫£ hai ph·∫≥ng d·∫ßn

üëâ Model ƒë√£ h·ªçc g·∫ßn h·∫øt th√¥ng tin h·ªØu √≠ch trong data

üîπ Giai ƒëo·∫°n sau (>500 step ‚Üí 1700 step)

Loss g·∫ßn nh∆∞ kh√¥ng gi·∫£m th√™m

Train & eval d√≠nh s√°t nhau

‚ö†Ô∏è ƒê√¢y l√† ƒëi·ªÉm quan tr·ªçng:

Model ƒëang train d∆∞ th·ª´a

3Ô∏è‚É£ C√≥ overfitting kh√¥ng?

‚ùå KH√îNG

D·∫•u hi·ªáu overfit l√†:

Train loss ‚Üì

Eval loss ‚Üë

üëâ ·ªû ƒë√¢y:

Train ‚Üì

Eval ‚Üì v√† ngang

=> Model generalize t·ªët

4Ô∏è‚É£ Nh∆∞ng c√≥ v·∫•n ƒë·ªÅ g√¨ kh√¥ng?

üëâ C√≥ 1 v·∫•n ƒë·ªÅ duy nh·∫•t:

‚õî B·∫°n train QU√Å L√ÇU

Sau ~300‚Äì500 step:

Loss kh√¥ng gi·∫£m ƒë√°ng k·ªÉ

Train ti·∫øp ch·ªâ t·ªën th·ªùi gian

Kh√¥ng c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng

5Ô∏è‚É£ K·∫øt lu·∫≠n nhanh (1 d√≤ng)

Model h·ªçc xong t·ª´ s·ªõm, b·∫°n ƒëang √©p n√≥ h·ªçc l·∫°i ki·∫øn th·ª©c c≈©

6Ô∏è‚É£ G·ª£i √Ω ch·ªânh config cho l·∫ßn sau (r·∫•t n√™n)
‚úÖ C√°ch 1: Gi·∫£m epoch
num_train_epochs = 1

‚úÖ C√°ch 2: Gi·ªõi h·∫°n step
max_steps = 500

‚úÖ C√°ch 3: Early stopping (chu·∫©n ch·ªânh)
from transformers import EarlyStoppingCallback

trainer.add_callback(
    EarlyStoppingCallback(
        early_stopping_patience=2
    )
)

7Ô∏è‚É£ N·∫øu ƒë√¢y l√† model embedding / SimCSE

## so s√°nh TTFT, ITPS, OET, ...
"""

import json
import random
import time
import torch
import psutil
from unsloth import FastLanguageModel
from transformers import TextStreamer
import numpy as np

# ============================
# 1. Load dataset + l·∫•y 10 m·∫´u
# ============================

json_path = "/content/drive/MyDrive/Quang Huy/MedicalChatbotFinetune/Dataset/pediatric_samples_v2.json"

with open(json_path, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# Random 10 samples
samples = random.sample(dataset, 10)
print("Selected 10 samples.")

print(samples[0])

# Convert dataset to prompt format
def build_prompt(sample):
    return f"{sample['instruction']}\n\nC√¢u h·ªèi: {sample['question']}\n\nTr·∫£ l·ªùi:"


# ============================
# Streamer t√≠nh token/time
# ============================
class TimingStreamer(TextStreamer):
    def __init__(self, tokenizer):
        super().__init__(tokenizer)
        self.token_count = 0
        self.first_token_time = None

    def on_finalized_text(self, text, stream_end=False):
        if self.token_count == 0:
            self.first_token_time = time.time()
        self.token_count += 1

# ============================
# 3. Benchmark t·ª´ng sample
# ============================
results = []

process = psutil.Process()

for i, sample in enumerate(samples):
    print(f"\nRunning sample {i+1}/10 ...")

    prompt = build_prompt(sample)

    # encode
    encoded = tokenizer(prompt, return_tensors="pt")
    input_ids = encoded.input_ids.cuda()

    # input token count
    input_token_len = input_ids.shape[1]

    # RAM before
    ram_before = process.memory_info().rss / (1024**3)

    # Streamer
    streamer = TimingStreamer(tokenizer)

    # Timing
    start_time = time.time()

    output = model.generate(
        input_ids,
        max_new_tokens=150,
        temperature=0.7,
        streamer=streamer
    )

    end_time = time.time()

    # Extract
    TTFT = streamer.first_token_time - start_time
    OET = end_time - start_time
    OTPS = streamer.token_count / OET
    ITPS = streamer.token_count / (end_time - streamer.first_token_time)

    # RAM & CPU
    ram_after = process.memory_info().rss / (1024**3)
    ram_used = ram_after - ram_before
    cpu_usage = process.cpu_percent(interval=0.2)

    results.append({
        "input_tokens": input_token_len,
        "TTFT": TTFT,
        "ITPS": ITPS,
        "OET": OET,
        "OTPS": OTPS,
        "Total Time": OET,
        "CPU%": cpu_usage,
        "RAM_GB": ram_used
    })


# ============================
# 4. T√≠nh mean gi·ªëng b√†i b√°o
# ============================

mean_tokens = np.mean([r["input_tokens"] for r in results])
mean_TTFT = np.mean([r["TTFT"] for r in results])
mean_ITPS = np.mean([r["ITPS"] for r in results])
mean_OET = np.mean([r["OET"] for r in results])
mean_OTPS = np.mean([r["OTPS"] for r in results])
mean_CPU = np.mean([r["CPU%"] for r in results])
mean_RAM = np.mean([r["RAM_GB"] for r in results])

print("\n================ FINAL BENCHMARK RESULTS ================")
print(f"Model: {model_name}")
print(f"Mean Input Tokens: {mean_tokens:.2f}")
print(f"TTFT(s): {mean_TTFT:.4f}")
print(f"ITPS(t/s): {mean_ITPS:.2f}")
print(f"OET(s): {mean_OET:.4f}")
print(f"OTPS(t/s): {mean_OTPS:.2f}")
print(f"CPU(%): {mean_CPU:.2f}")
print(f"RAM(GB): {mean_RAM:.3f}")
print("========================================================\n")

"""## L∆∞u model"""

save_path = "/content/drive/Shareddrives/NCKH_MEDICAL_CHATBOT/SourceTrain_Huy/OutputModel/parent_bot_ft"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("ƒê√£ l∆∞u model v√† tokenizer v√†o:", save_path)